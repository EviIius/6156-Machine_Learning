{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jzQYGS7LRHJ2y0z0hbwPgKfyopuEXjT_","timestamp":1698780303445},{"file_id":"1zSOW-AgO7-tj-ARyonXUpoM06TGFEihl","timestamp":1698773996131}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **DSBA 6165 Applied Machine Learning**\n","# CNN Lab\n","\n","Professor: Rick Chakra\n","\n","TA: Geethika Balasubramanian"],"metadata":{"id":"qxbefL-p2bsl"}},{"cell_type":"markdown","source":["## MNIST CNN"],"metadata":{"id":"qezUc9xjVrP4"}},{"cell_type":"markdown","source":["This code imports necessary libraries and sets up the environment for training a PyTorch model on a GPU if available"],"metadata":{"id":"DoKBVaQWD0sA"}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","from torchvision import datasets, transforms\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import time\n","import numpy as np\n","\n","#import helper\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","# Load tensorboard\n","from torch.utils.tensorboard import SummaryWriter\n","writer = #Fill IN\n","\n","# moves your model to train on your gpu if available else it uses your cpu\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# check GPU properties\n","print(torch.cuda.get_device_name())\n","print(torch.cuda.get_device_properties(torch.cuda.device))"],"metadata":{"id":"c-P76wT1D2bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we finally download the data sets, shuffle them and transform each of them. We download the data sets and load them to DataLoader, which combines the data-set and a sampler and provides single- or multi-process iterators over the data-set."],"metadata":{"id":"MyLY10Br724u"}},{"cell_type":"markdown","source":["Torchvision datasets module has many built-in datsets https://pytorch.org/vision/stable/datasets.html"],"metadata":{"id":"328_gJRRo-PV"}},{"cell_type":"code","source":["max_degree = #FILL IN\n","# Define transform to normalize data\n","train_transform = transforms.Compose([#FILL IN,\n","                                transforms.ToTensor(),\n","                                transforms.Normalize((0.1307,),(0.3081,))\n","                                ])\n","\n","test_transform = transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Normalize((0.1307,),(0.3081,))\n","                                ])\n","\n","# Download and load the training data\n","train_set = datasets.MNIST('DATA_MNIST/', download=True, train=True, transform=#FILL IN)\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n","\n","test_set = datasets.MNIST('DATA_MNIST/', download=True, train=False, transform=#FILL IN)\n","testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"],"metadata":{"id":"F9HCGG2573Dl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once this cell is executed, our dataset is downloaded and stored in the variable train_set and test_set. In order to load the MNIST dataset in a handy way, we will need DataLoaders for the dataset. We will use a batch_size of 64 for the training.\n","\n","The values 0.1307 and 0.3081 used for transforms.Normalize() transformation represents the global mean and standard deviation of the MNIST dataset and transforms.ToTensor()converts the entire array into torch tensor and divides by 255. So values are between 0.0f and 1.0f."],"metadata":{"id":"iJUuLBri75Ec"}},{"cell_type":"code","source":["training_data = enumerate(trainloader)"],"metadata":{"id":"c8lmNroQFOBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_idx, (images, labels) = next(training_data)\n","print(type(images)) # Checking the datatype\n","print(batch_idx) # batch id\n","print(images.shape) # the size of the image\n","print(labels.shape) # the size of the labels"],"metadata":{"id":"CMSvNOeU75L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let’s turn our trainloader object into an iterator with iter so we may access our images and labels from this generator. We can see the shape as (64 x 1 x 28 x 28). This means:\n","\n","* 64: Represents 64 images (batch)\n","* 1 : One color channel ==>> Grayscale\n","* 28 by 28 pixel: the shape of these images so we can visualize it."],"metadata":{"id":"k8CE0Cz67_Fs"}},{"cell_type":"code","source":["fig = plt.figure()\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    plt.tight_layout()\n","    plt.imshow(images[i][0], cmap='gray') # try cmap='inferno'\n","    plt.title(\"Ground Truth Label: {}\".format(labels[i]))\n","    plt.yticks([])\n","    plt.xticks([])"],"metadata":{"id":"HzXgsCof8B7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let’s build our network using this object-oriented class method within nn.module. We will use 2 fully convolutional layers, Relu activation function and MaxPooling. This will also be coupled along with 2 linear layers with a dropout probability of 0.2 per cent."],"metadata":{"id":"6ZEwH1eP8EOW"}},{"cell_type":"code","source":["class Network(nn.Module):\n","\n","    def __init__(self):\n","        super(Network, self).__init__()\n","        # Convolutional Neural Network Layer\n","        self.convolutaional_neural_network_layers = #FILL IN(\n","                # Here we are defining our 2D convolutional layers\n","                # We can calculate the output size of each convolutional layer using the following formula\n","                # outputOfEachConvLayer = [(in_height + 2*padding - kernel_size) / stride] + 1\n","                # We have in_channels=1 because our input is a grayscale image\n","                nn.Conv2d(#FILL IN),\n","                #FILL IN#,\n","                # After the first convolutional layer the output of this layer is:\n","                # [(28 + 2*1 - 3)/1] + 1 = 28.\n","                nn.MaxPool2d(#FILL IN),\n","                # Since we applied maxpooling with kernel_size=2 we have to divide by 2, so we get\n","                # 28 / 2 = 14\n","\n","                # output of our second conv layer\n","                nn.Conv2d(#fill in),\n","                #FILL IN#,\n","                # After the second convolutional layer the output of this layer is:\n","                # [(14 + 2*1 - 3)/1] + 1 = 14.\n","                nn.MaxPool2d(#FILL IN)\n","                # Since we applied maxpooling with kernel_size=2 we have to divide by 2, so we get\n","                # 14 / 2 = 7\n","        )\n","\n","        # Linear layer\n","        self.linear_layers = #FILL IN(\n","                # We have the output_channel=24 of our second conv layer, and 7*7 is derived by the formula\n","                # which is the output of each convolutional layer\n","                nn.Linear(#FILL IN),\n","                #FILL IN#,\n","                nn.Dropout(p=0.2), # Dropout with probability of 0.2 to avoid overfitting\n","                nn.Linear(in_features=64, out_features=10) # The output is 10 which should match the size of our class\n","        )\n","\n","    # Defining the forward pass\n","    def forward(self, x):\n","        x = self.convolutaional_neural_network_layers(x)\n","        # After we get the output of our convolutional layer we must flatten it or rearrange the output into a vector\n","        x = x.view(x.size(0), -1)\n","        # Then pass it through the linear layer\n","        x = self.linear_layers(x)\n","        return x"],"metadata":{"id":"Yf08mpWZ8EpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The forward() pass defines the way our output is being computed. The line x.view(x.size(0), -1) flattens the output from the convolution layer into a vector. Most of the time when experimenting with more complex models it is advised-able to print out the tensor values for easier debugging.\n","\n","Now by initializing our network the term model.to(device) sends the network we created into Cuda. Note: This only happens if you have a GPU. This helps to decrease the time it takes our network to train, then we define the optimizers we will use."],"metadata":{"id":"Y0hWzgw_8I5l"}},{"cell_type":"code","source":["model = Network()\n","model.to(device)"],"metadata":{"id":"NiSSdG8P8MEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = #FILL IN#(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"yq5c5QBu8NvF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After the forward pass, a loss function is calculated from the target output and the prediction labels in order to update weights for the best model selection in the further step. Setting up the loss function is a fairly simple step in PyTorch. Here, we will use the Cross–entropy loss, or log loss, which measures the performance of a classification model whose output is a probability value between 0 and 1. We should note that the Cross–entropy loss increases as the predicted probability diverges from the actual label.\n","\n","Next, we will use Stochastic Gradient Descent optimizer for the update of hyper-parameters model.parameters() will provide the learnable parameters to the optimizer and lr=0.01 defines the learning rates for the parameter updates."],"metadata":{"id":"TIrHVhD08PcM"}},{"cell_type":"markdown","source":["## 4. Training and Testing the Model"],"metadata":{"id":"-2npK4r68RWu"}},{"cell_type":"markdown","source":["Our model is now ready to train. We begin by setting up an epoch size. Epoch is a single pass through the whole training dataset. In the example below, the epoch size is set to 10, meaning there will be 10 single passes of the training and weight updates."],"metadata":{"id":"HUWdB-Yy8VjF"}},{"cell_type":"code","source":["print('Starting Training')\n","epochs = #FILL IN # The total number of epochs\n","\n","train_losses = []\n","test_losses = []\n","\n","t0 = time.time()\n","\n","for epoch in range(epochs):\n","    t1 = time.time()\n","    # prep model for training\n","    model.train()\n","    train_loss = 0\n","\n","    for idx, (images, labels) in enumerate(trainloader):\n","\n","        # Send these >>> To GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Training pass\n","        #FILL IN\n","\n","        # Forward pass\n","        #FILL IN\n","        loss = criterion(output, labels)\n","\n","        #Backward pass\n","        #FILL IN\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # prep model for evaluation\n","    #FILL IN\n","    test_loss = 0\n","    accuracy = 0\n","\n","    # Turn off the gradients when performing validation.\n","    # If we don't turn it off, we will comprise our networks weight entirely\n","    with torch.no_grad():\n","        for images, labels in testloader:\n","\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            log_probabilities = model(images)\n","            test_loss += criterion(log_probabilities, labels)\n","\n","            probabilities = torch.exp(log_probabilities)\n","            top_prob, top_class = probabilities.topk(1, dim=1)\n","            predictions = top_class == labels.view(*top_class.shape)\n","            accuracy += torch.mean(predictions.type(torch.FloatTensor))\n","\n","    train_losses.append(train_loss/len(trainloader))\n","    writer.add_scalar('training loss', train_loss/len(trainloader), epoch+1)\n","    test_losses.append((test_loss/len(testloader)).cpu().numpy())\n","    writer.add_scalar('testing loss', (test_loss/len(testloader)).cpu().numpy(), epoch+1)\n","\n","    print(\"Epoch: {}/{}  \".format(epoch+1, epochs),\n","          \"Training loss: {:.4f}  \".format(train_loss/len(trainloader)),\n","          \"Testing loss: {:.4f}  \".format(test_loss/len(testloader)),\n","          \"Test accuracy: {:.4f}  \".format(accuracy/len(testloader)),\n","          \"Epoch time: {:.4f}  \".format(time.time() - t1))\n","\n","print(\"Training Complete\")\n","print(\"Total Elapsed Time: {:.4f} \".format(time.time() - t0))\n","writer.flush()"],"metadata":{"id":"KPi_YKc98YC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After the forward pass and the loss, computation is done, we do a backward pass, which refers to the process of learning and updating the weights. We first need to set our gradient to zero: optimizer.zero_grad(). This is because every time a variable is backpropagated through the network multiple times, the gradient will be accumulated instead of being replaced from the previous training step in our current training step. Which will prevent our network from learning properly. Then we run a backward pass by loss.backward() and optimizer.step() which updates our parameters based on the current gradient.\n","\n","By training our network we may also test our model to see how it’s performing after each epoch. The most crucial method is to set model.eval() when you want to test your network to avoid updating the gradient during testing and when you want to start training, set model.train(), so your weights may be updated."],"metadata":{"id":"bn5Zetrq8aCk"}},{"cell_type":"code","source":["\n","# plot the training and test losses\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(np.array([test_loss for test_loss in test_losses]), label='Test loss')\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"_MgfKa868byM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let’s check our training and validation accuracy. It is as simple as the code for plotting the loss."],"metadata":{"id":"fm80aEMd8d3U"}},{"cell_type":"markdown","source":["## 5. Evaluating the Network"],"metadata":{"id":"iQwYCBV58hoG"}},{"cell_type":"code","source":["# select index of image to test\n","img = images[11]\n","img = img.to(device)\n","img = img.view(-1, 1, 28, 28)\n","print(img.shape)\n","\n","# Since we want to use the already pretrained weights to make some prediction\n","# we are turning off the gradients\n","with torch.no_grad():\n","    logits = model(img)"],"metadata":{"id":"JKA-aXiQ8jUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And by passing these logits through a softmax function we get probabilities values as our output. Let’s visualize this."],"metadata":{"id":"wISLDv328mf9"}},{"cell_type":"code","source":["#We take the softmax for probabilites since our outputs are logits\n","probabilities = F.softmax(logits, dim=1).detach().cpu().numpy().squeeze()\n","\n","print(probabilities)\n","\n","fig, (ax1, ax2) = plt.subplots(figsize=(6,8), ncols=2)\n","ax1.imshow(img.view(1, 28, 28).detach().cpu().numpy().squeeze(), cmap='inferno')\n","ax1.axis('off')\n","ax2.barh(np.arange(10), probabilities, color='r' )\n","ax2.set_aspect(0.1)\n","ax2.set_yticks(np.arange(10))\n","ax2.set_yticklabels(np.arange(10))\n","ax2.set_title('Class Probability')\n","ax2.set_xlim(0, 1.1)\n","\n","plt.tight_layout()"],"metadata":{"id":"6hwpl6XS8ock"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the input image 7 matches the probability with the highest value in the probability class figure.\n","\n","Summary\n","To sum it up, the training pass consists of four different steps. Which are:\n","\n","* First, make a forward pass through the network.\n","* Use the network output to calculate the loss.\n","* Perform a backward pass through the network with loss.backwards() and this calculates the gradients.\n","* Then we make a step with our optimizer which updates the weights.\n","\n","Now we have learned how to train and test our model to make accurate predictions for the digit dataset. In the next tutorial we will experiment more on different datasets."],"metadata":{"id":"D0NIJP8G8rfU"}},{"cell_type":"code","source":["%reload_ext #FILL IN\n","%tensorboard #FILL IN"],"metadata":{"id":"rB9I8IXHfDnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## References\n","- [Building RNNs is Fun with PyTorch and Google Colab](https://colab.research.google.com/drive/1NVuWLZ0cuXPAtwV4Fs2KZ2MNla0dBUas)\n","- [CNN Basics with PyTorch by Sebastian Raschka](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb)\n","- [Tensorflow 2.0 Quickstart for experts](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=DUNzJc4jTj6G)"],"metadata":{"id":"CyW1BluIg_GT"}}]}