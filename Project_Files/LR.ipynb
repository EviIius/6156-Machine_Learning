{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                28831\n",
      "Model:                          Logit   Df Residuals:                    28803\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Mon, 15 Apr 2024   Pseudo R-squ.:                 0.04185\n",
      "Time:                        21:55:56   Log-Likelihood:                -9730.6\n",
      "converged:                       True   LL-Null:                       -10156.\n",
      "Covariance Type:            nonrobust   LLR p-value:                1.093e-161\n",
      "=================================================================================================\n",
      "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.2029      3.046      0.067      0.947      -5.768       6.174\n",
      "age                               0.0102      0.002      4.419      0.000       0.006       0.015\n",
      "cons.price.idx                   -0.0279      0.032     -0.860      0.390      -0.092       0.036\n",
      "campaign                         -0.1323      0.011    -11.676      0.000      -0.155      -0.110\n",
      "job_blue-collar                  -0.4144      0.075     -5.543      0.000      -0.561      -0.268\n",
      "job_entrepreneur                 -0.3936      0.120     -3.269      0.001      -0.630      -0.158\n",
      "job_housemaid                    -0.2406      0.138     -1.750      0.080      -0.510       0.029\n",
      "job_management                   -0.1886      0.080     -2.351      0.019      -0.346      -0.031\n",
      "job_retired                       0.7232      0.096      7.535      0.000       0.535       0.911\n",
      "job_self-employed                -0.2715      0.112     -2.434      0.015      -0.490      -0.053\n",
      "job_services                     -0.2798      0.081     -3.450      0.001      -0.439      -0.121\n",
      "job_student                       1.0805      0.102     10.604      0.000       0.881       1.280\n",
      "job_technician                   -0.1567      0.066     -2.388      0.017      -0.285      -0.028\n",
      "job_unemployed                    0.2257      0.115      1.958      0.050      -0.000       0.452\n",
      "job_unknown                      -0.1602      0.219     -0.732      0.464      -0.589       0.269\n",
      "education_basic.6y               -0.0258      0.114     -0.227      0.820      -0.248       0.197\n",
      "education_basic.9y               -0.1188      0.089     -1.329      0.184      -0.294       0.056\n",
      "education_high.school             0.0367      0.086      0.429      0.668      -0.131       0.204\n",
      "education_illiterate              0.0608      0.974      0.062      0.950      -1.849       1.971\n",
      "education_professional.course     0.1415      0.094      1.498      0.134      -0.044       0.327\n",
      "education_university.degree       0.3377      0.085      3.971      0.000       0.171       0.504\n",
      "education_unknown                 0.1723      0.111      1.546      0.122      -0.046       0.391\n",
      "marital_married                   0.0871      0.064      1.351      0.177      -0.039       0.213\n",
      "marital_single                    0.3786      0.073      5.212      0.000       0.236       0.521\n",
      "marital_unknown                   0.0618      0.475      0.130      0.896      -0.868       0.992\n",
      "housing_unknown                   0.0074   4.19e+06   1.77e-09      1.000   -8.22e+06    8.22e+06\n",
      "housing_yes                       0.0370      0.039      0.957      0.339      -0.039       0.113\n",
      "loan_unknown                      0.0074   4.19e+06   1.77e-09      1.000   -8.22e+06    8.22e+06\n",
      "loan_yes                          0.0156      0.053      0.297      0.767      -0.088       0.119\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Otherdata/bank-additional-full.csv', delimiter=';')\n",
    "\n",
    "# Select specific columns for the model\n",
    "selected_columns = ['age', 'job', 'education', 'marital', 'housing', 'loan', 'cons.price.idx', 'campaign', 'y']\n",
    "selected_data = data[selected_columns]\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['job', 'education', 'marital', 'housing', 'loan']  # List of categorical columns excluding 'y'\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "encoded_categorical = encoder.fit_transform(selected_data[categorical_cols]).toarray()\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "numerical_features = selected_data[['age', 'cons.price.idx', 'campaign']]  # Explicitly list numerical features\n",
    "combined_features = pd.concat([numerical_features, encoded_categorical_df], axis=1)\n",
    "\n",
    "# Prepare target variable by converting 'y' to binary\n",
    "target = selected_data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Add constant to training data for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit logistic regression model with a method that includes regularization\n",
    "model = sm.Logit(y_train, X_train_sm)\n",
    "result = model.fit(method='lbfgs', maxiter=500)  # Increased maxiter and changed method to 'lbfgs'\n",
    "\n",
    "# Print the model summary\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                28831\n",
      "Model:                          Logit   Df Residuals:                    28803\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Mon, 15 Apr 2024   Pseudo R-squ.:                 0.04098\n",
      "Time:                        21:55:57   Log-Likelihood:                -9739.5\n",
      "converged:                       True   LL-Null:                       -10156.\n",
      "Covariance Type:            nonrobust   LLR p-value:                6.233e-158\n",
      "=================================================================================================\n",
      "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             0.0735      2.978      0.025      0.980      -5.763       5.910\n",
      "age                               0.0094      0.002      4.094      0.000       0.005       0.014\n",
      "cons.price.idx                   -0.0238      0.032     -0.753      0.452      -0.086       0.038\n",
      "campaign                         -0.1344      0.011    -11.841      0.000      -0.157      -0.112\n",
      "job_admin.                        0.0177      0.209      0.085      0.933      -0.392       0.427\n",
      "job_blue-collar                  -0.4099      0.212     -1.937      0.053      -0.825       0.005\n",
      "job_entrepreneur                 -0.3797      0.234     -1.621      0.105      -0.839       0.079\n",
      "job_housemaid                    -0.2165      0.241     -0.897      0.369      -0.689       0.256\n",
      "job_management                   -0.1967      0.218     -0.904      0.366      -0.623       0.230\n",
      "job_retired                       0.7828      0.219      3.581      0.000       0.354       1.211\n",
      "job_self-employed                -0.2723      0.231     -1.179      0.238      -0.725       0.180\n",
      "job_services                     -0.3066      0.217     -1.411      0.158      -0.733       0.119\n",
      "job_student                       1.1114      0.225      4.940      0.000       0.670       1.552\n",
      "job_technician                   -0.1389      0.212     -0.655      0.512      -0.554       0.277\n",
      "job_unemployed                    0.2196      0.232      0.946      0.344      -0.236       0.675\n",
      "education_basic.4y               -0.1534      0.113     -1.362      0.173      -0.374       0.067\n",
      "education_basic.6y               -0.1366      0.130     -1.048      0.295      -0.392       0.119\n",
      "education_basic.9y               -0.2443      0.109     -2.238      0.025      -0.458      -0.030\n",
      "education_high.school            -0.0861      0.099     -0.868      0.386      -0.281       0.108\n",
      "education_illiterate              0.0432      0.942      0.046      0.963      -1.803       1.889\n",
      "education_professional.course     0.0118      0.108      0.109      0.913      -0.200       0.224\n",
      "education_university.degree       0.2195      0.098      2.239      0.025       0.027       0.412\n",
      "marital_divorced                 -0.1369      0.458     -0.299      0.765      -1.035       0.761\n",
      "marital_married                  -0.0512      0.455     -0.112      0.910      -0.943       0.841\n",
      "marital_single                    0.2292      0.456      0.503      0.615      -0.664       1.122\n",
      "housing_no                        0.0048   5.45e+06   8.81e-10      1.000   -1.07e+07    1.07e+07\n",
      "housing_yes                       0.0381   5.45e+06   6.99e-09      1.000   -1.07e+07    1.07e+07\n",
      "loan_no                           0.0154   5.45e+06   2.83e-09      1.000   -1.07e+07    1.07e+07\n",
      "loan_yes                          0.0275   5.45e+06   5.04e-09      1.000   -1.07e+07    1.07e+07\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Otherdata/bank-additional-full.csv', delimiter=';')\n",
    "\n",
    "# Select specific columns for the model\n",
    "selected_columns = ['age', 'job', 'education', 'marital', 'housing', 'loan', 'cons.price.idx', 'campaign', 'y']\n",
    "selected_data = data[selected_columns]\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['job', 'education', 'marital', 'housing', 'loan']  # List of categorical columns excluding 'y'\n",
    "encoder = OneHotEncoder()\n",
    "encoded_categorical = encoder.fit_transform(selected_data[categorical_cols]).toarray()\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "numerical_features = selected_data[['age', 'cons.price.idx', 'campaign']]  # Explicitly list numerical features\n",
    "combined_features = pd.concat([numerical_features, encoded_categorical_df], axis=1)\n",
    "\n",
    "combined_features = combined_features.drop(['housing_unknown', 'loan_unknown', 'job_unknown', 'education_unknown', 'marital_unknown'], axis=1).astype(int) # Drop unknown columns\n",
    "\n",
    "# Prepare target variable by converting 'y' to binary\n",
    "target = selected_data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Add constant to training data for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit logistic regression model with a method that includes regularization\n",
    "model = sm.Logit(y_train, X_train_sm)\n",
    "result = model.fit(method='lbfgs', maxiter=500)  # Increased maxiter and changed method to 'lbfgs'\n",
    "\n",
    "# Print the model summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                51160\n",
      "Model:                          Logit   Df Residuals:                    51132\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Mon, 15 Apr 2024   Pseudo R-squ.:                 0.07370\n",
      "Time:                        21:55:59   Log-Likelihood:                -32848.\n",
      "converged:                      False   LL-Null:                       -35461.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=================================================================================================\n",
      "                                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "const                             2.1163      1.478      1.432      0.152      -0.780       5.013\n",
      "age                               0.0070      0.001      6.108      0.000       0.005       0.009\n",
      "cons.price.idx                   -0.0226      0.016     -1.431      0.152      -0.053       0.008\n",
      "campaign                         -0.2010      0.006    -35.007      0.000      -0.212      -0.190\n",
      "job_blue-collar                  -0.5672      0.037    -15.202      0.000      -0.640      -0.494\n",
      "job_entrepreneur                 -0.9142      0.062    -14.629      0.000      -1.037      -0.792\n",
      "job_housemaid                    -0.6971      0.072     -9.702      0.000      -0.838      -0.556\n",
      "job_management                   -0.5041      0.042    -12.133      0.000      -0.586      -0.423\n",
      "job_retired                       0.7220      0.057     12.732      0.000       0.611       0.833\n",
      "job_self-employed                -0.7530      0.059    -12.685      0.000      -0.869      -0.637\n",
      "job_services                     -0.6026      0.041    -14.862      0.000      -0.682      -0.523\n",
      "job_student                       1.0090      0.064     15.790      0.000       0.884       1.134\n",
      "job_technician                   -0.3302      0.034     -9.756      0.000      -0.397      -0.264\n",
      "job_unemployed                   -0.2128      0.066     -3.224      0.001      -0.342      -0.083\n",
      "job_unknown                      -1.4501      0.137    -10.550      0.000      -1.719      -1.181\n",
      "education_basic.6y               -0.1592      0.057     -2.791      0.005      -0.271      -0.047\n",
      "education_basic.9y               -0.2027      0.045     -4.531      0.000      -0.290      -0.115\n",
      "education_high.school             0.0286      0.045      0.642      0.521      -0.059       0.116\n",
      "education_illiterate              0.2871      0.490      0.585      0.558      -0.674       1.248\n",
      "education_professional.course     0.1164      0.050      2.334      0.020       0.019       0.214\n",
      "education_university.degree       0.4036      0.045      8.903      0.000       0.315       0.492\n",
      "education_unknown                 0.0487      0.061      0.803      0.422      -0.070       0.167\n",
      "marital_married                   0.2436      0.033      7.305      0.000       0.178       0.309\n",
      "marital_single                    0.4170      0.038     10.971      0.000       0.343       0.492\n",
      "marital_unknown                   0.2317      0.261      0.889      0.374      -0.279       0.743\n",
      "housing_unknown                  -0.2397   2.97e+06  -8.08e-08      1.000   -5.81e+06    5.81e+06\n",
      "housing_yes                       0.0404      0.020      2.062      0.039       0.002       0.079\n",
      "loan_unknown                     -0.2397   2.97e+06  -8.08e-08      1.000   -5.81e+06    5.81e+06\n",
      "loan_yes                         -0.2015      0.028     -7.146      0.000      -0.257      -0.146\n",
      "=================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rbrul\\Documents\\GitHub\\6156-Machine_Learning\\.venv\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Otherdata/bank-additional-full.csv', delimiter=';')\n",
    "\n",
    "# Select specific columns for the model\n",
    "selected_columns = ['age', 'job', 'education', 'marital', 'housing', 'loan', 'cons.price.idx', 'campaign', 'y']\n",
    "selected_data = data[selected_columns]\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = ['job', 'education', 'marital', 'housing', 'loan']  # List of categorical columns excluding 'y'\n",
    "\n",
    "# Initialize encoder with specifying to drop the first category in each feature\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "encoded_categorical = encoder.fit_transform(selected_data[categorical_cols]).toarray()\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "numerical_features = selected_data[['age', 'cons.price.idx', 'campaign']]  # Explicitly list numerical features\n",
    "combined_features = pd.concat([numerical_features, encoded_categorical_df], axis=1)\n",
    "\n",
    "# Prepare target variable by converting 'y' to binary\n",
    "target = selected_data['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit logistic regression model using statsmodels (for detailed stats) or scikit-learn (for simplicity)\n",
    "model = sm.Logit(y_train_smote, sm.add_constant(X_train_smote))\n",
    "result = model.fit(method='lbfgs', maxiter=500)\n",
    "\n",
    "# Alternatively, use scikit-learn logistic regression if detailed statistics are not needed\n",
    "# lr = LogisticRegression(max_iter=500)\n",
    "# lr.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on test data\n",
    "# y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print model summary\n",
    "print(result.summary())\n",
    "\n",
    "# Evaluate the model\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
