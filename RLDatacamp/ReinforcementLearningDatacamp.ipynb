{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 1: Introduction to Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Question 1: \n",
    "- **RL is based on rewarding desireable behaviors and punishing undesireable ones**\n",
    "- **RL is based on the interaction between an agent and an enviorment to achieve a specific goal** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "![A test image](Question2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Question 3:\n",
    "## ![A test image](Question3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Question 4: \n",
    "![A test image](Question4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: \n",
    "![A test image](Question5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6.1\n",
    "exp_rewards_strategy_1 = np.array([3, 2, -1, 5])\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Compute discounts\n",
    "discounts_strategy_1 = np.array([discount_factor**i for i in range(len(exp_rewards_strategy_1))])\n",
    "\n",
    "# Compute the discounted return\n",
    "discounted_return_strategy_1 = np.sum(exp_rewards_strategy_1 * discounts_strategy_1)\n",
    "\n",
    "print(f\"The discounted return of the first strategy is {discounted_return_strategy_1}\")\n",
    "\n",
    "# Question 6.2\n",
    "exp_rewards_strategy_2 = np.array([6, -5, -3, -2])\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Compute discounts\n",
    "discounts_strategy_2 = np.array([discount_factor**i for i in range(len(exp_rewards_strategy_2))])\n",
    "\n",
    "# Compute the discounted return\n",
    "discounted_return_strategy_2 = np.sum(exp_rewards_strategy_2 * discounts_strategy_2)\n",
    "\n",
    "print(f\"The discounted return of the second strategy is {discounted_return_strategy_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gymnasium library\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MountainCar', render_mode = 'rgb_array')\n",
    "\n",
    "# Get the initial state\n",
    "initial_state, info = env.reset(seed=42)\n",
    "\n",
    "position = initial_state[0]\n",
    "velocity = initial_state[1]\n",
    "\n",
    "print(f\"The position of the car along the x-axis is {position} (m)\")\n",
    "print(f\"The velocity of the car is {velocity} (m/s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar', render_mode='rgb_array')\n",
    "initial_state, _ = env.reset()\n",
    "\n",
    "# Complete the render function\n",
    "def render():\n",
    "    state_image = env.render()\n",
    "    plt.imshow(state_image)\n",
    "    plt.show()\n",
    "\n",
    "# Call the render function    \n",
    "render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence of actions\n",
    "actions = [2,2,1,1,1,2]\n",
    "\n",
    "for action in actions:\n",
    "  # Execute each action\n",
    "  state, reward, terminated, _, _ = env.step(action)\n",
    "  # Render the environment\n",
    "  render()\n",
    "  if terminated:\n",
    "  \tprint(\"You reached the goal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 2: Model-Based Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question 1.1: 4\n",
    "- Question 1.2: 18\n",
    "- Question 1.3: 6\n",
    "- Question 1.4: Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the Cliff Walking environment\n",
    "env = gym.make('CliffWalking')\n",
    "\n",
    "# Compute the size of the action space\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Compute the size of the state space\n",
    "num_states = env.observation_space.n\n",
    "\n",
    "print(\"Number of actions:\", num_actions)\n",
    "print(\"Number of states:\", num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3.1\n",
    "# Choose the state\n",
    "state = 35  # or any specific state you want to examine\n",
    "\n",
    "# Compute the size of the action space\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Extract transitions for each state-action pair\n",
    "for action in range(num_actions):\n",
    "    transitions = env.unwrapped.P[state][action]\n",
    "    # Print details of each transition\n",
    "    for transition in transitions:\n",
    "        probability, next_state, reward, done = transition\n",
    "        print(f\"Probability: {probability}, Next State: {next_state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3.2: Deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make('MyGridWorld', render_mode = 'rgb_array')\n",
    "state, info = env.reset()\n",
    "\n",
    "# Define the policy\n",
    "policy = {0: 2, 1: 2, 2: 1, 3: 1, 4: 0, 5: 0, 6: 2, 7: 2}\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "  # Select action based on policy \n",
    "  action = policy[state]\n",
    "  state, reward, terminated, truncated, info = env.step(action)\n",
    "  # Render the environment\n",
    "  render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function\n",
    "def compute_state_value(state):\n",
    "    if state == terminal_state:\n",
    "        return 0\n",
    "    action = policy[state]\n",
    "    _, next_state, reward, _ = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state) \n",
    "\n",
    "# Compute all state values \n",
    "state_values = {state: compute_state_value(state) for state in range(num_states)}\n",
    "\n",
    "print(state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function_1 = {0: 1, 1: 2, 2: 3, 3: 7, 4: 6, 5: 4, 6: 8, 7: 10, 8: 0}\n",
    "value_function_2 = {0: 7, 1: 8, 2: 9, 3: 7, 4: 9, 5: 10, 6: 8, 7: 10, 8: 0}\n",
    "\n",
    "# Check for each value in policy 1 if it is better than policy 2\n",
    "one_is_better = [value_function_1[state] >= value_function_2[state] for state in range(num_states)]\n",
    "\n",
    "# Check for each value in policy 2 if it is better than policy 1\n",
    "two_is_better = [value_function_2[state] >= value_function_1[state] for state in range(num_states)]\n",
    "\n",
    "if all(one_is_better):\n",
    "  print(\"Policy 1 is better.\")\n",
    "elif all(two_is_better):\n",
    "  print(\"Policy 2 is better.\")\n",
    "else:\n",
    "  print(\"Neither policy is uniformly better across all states.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the function to compute the action-value for a state-action pair\n",
    "def compute_q_value(state, action):\n",
    "    if state == terminal_state:\n",
    "        return None   \n",
    "    probability, next_state, reward, done = env.unwrapped.P[state][action][0]\n",
    "    return reward + gamma * compute_state_value(next_state)\n",
    "\n",
    "\n",
    "# Compute Q-values for each state-action pair\n",
    "Q = {(state, action): compute_q_value(state,action)\n",
    "\t  for state in range(num_states) for action in range(num_actions)}\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_policy = {}\n",
    "\n",
    "for state in range(num_states-1):\n",
    "    # Find the best action for each state based on Q-values\n",
    "    max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "    improved_policy[state] = max_action\n",
    "\n",
    "terminated = False\n",
    "while not terminated:\n",
    "  # Select action based on policy \n",
    "  action = improved_policy[state]\n",
    "  # Execute the action\n",
    "  state, reward, terminated, _, _ = env.step(action)\n",
    "  render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9.1\n",
    "# Complete the policy evaluation function\n",
    "def policy_evaluation(policy):\n",
    "    V = {state:compute_state_value(state,policy)for state in range(num_states)}\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9.2\n",
    "def policy_improvement(policy):\n",
    "    improved_policy = {s: 0 for s in range(num_states-1)}\n",
    "    \n",
    "\t# Compute the Q-value for each state-action pair\n",
    "    Q = {(state, action): compute_q_value(state, action, policy)\n",
    "    for state in range(num_states) for action in range(num_actions)}\n",
    "            \n",
    "    # Compute the new policy based on the Q-values\n",
    "    for state in range(num_states-1):\n",
    "        max_action = max(range(num_actions), key=lambda action: Q[(state, action)])\n",
    "        improved_policy[state] = max_action\n",
    "        \n",
    "    return improved_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 9.3\n",
    "# Complete the policy iteration function\n",
    "def policy_iteration():\n",
    "    policy = {0:2, 1:2, 2:1, 3:1, 4:0, 5:0, 6:2, 7:2}\n",
    "    while True:\n",
    "        V = policy_evaluation(policy)\n",
    "        improved_policy = policy_improvement(policy)\n",
    "        if improved_policy == policy:\n",
    "            break\n",
    "        policy = improved_policy\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "policy, V = policy_iteration()\n",
    "render_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.001\n",
    "while True:\n",
    "  new_V = {state: 0 for state in range(num_states)}\n",
    "  for state in range(num_states-1):\n",
    "    # Get action with maximum Q-value and its value \n",
    "    max_action, max_q_value = get_max_action_and_value(state, V)\n",
    "    # Update the value function and policy\n",
    "    new_V[state] = max_q_value\n",
    "    policy[state] = max_action\n",
    "  # Test if change in state values is negligeable\n",
    "  if all(abs(new_V[state] - V[state]) < threshold for state in V):\n",
    "    break\n",
    "  V = new_V\n",
    "render_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 3: Model-Based Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode():\n",
    "    episode = []\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "      # Select a random action\n",
    "      action = env.action_space.sample()\n",
    "      next_state, reward, terminated, truncated, info = env.step(action)\n",
    "      render()\n",
    "      # Update episode data\n",
    "      episode.append((state, action, reward))\n",
    "      state = next_state\n",
    "    return episode\n",
    "print(generate_episode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "  episode = generate_episode()\n",
    "  visited_states = set()\n",
    "  for j, (state, action, reward) in enumerate(episode):\n",
    "    # Define the first-visit condition\n",
    "    if (state,action) not in visited_states:\n",
    "      # Update the returns, their counts and the visited states\n",
    "      returns_sum[state, action] += sum([x[2] for x in episode[j:]])\n",
    "      returns_count[state, action] += 1\n",
    "      visited_states.add((state, action))\n",
    "\n",
    "nonzero_counts = returns_count != 0\n",
    "# Update the Q-values for visited state-action pairs\n",
    "Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "render_policy(get_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((num_states, num_actions))\n",
    "for i in range(100):\n",
    "  # Generate an episode\n",
    "  episode = generate_episode()\n",
    "  # Update the returns and their counts\n",
    "  for j, (state, action, reward) in enumerate(episode):\n",
    "    returns_sum[(state,  action)] += sum([x[2] for x in episode[j:]])\n",
    "    returns_count[(state,  action)] += 1\n",
    "\n",
    "# Update the Q-values for visited state-action pairs \n",
    "nonzero_counts = returns_count != 0\n",
    "Q[nonzero_counts] = returns_sum[nonzero_counts] / returns_count[nonzero_counts]\n",
    "    \n",
    "render_policy(get_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, next_state, next_action):\n",
    "  \t# Get the old value of the current state-action pair\n",
    "    old_value = Q[state, action]\n",
    "    # Get the value of the next state-action pair\n",
    "    next_value = Q[next_state, next_action]\n",
    "    # Compute the new value of the current state-action pair\n",
    "    Q[(state, action)] = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n",
    "\n",
    "alpha = 0.1\n",
    "gamma  = 0.8\n",
    "Q = np.array([[10,0],[0,20]], dtype='float32')\n",
    "# Update the Q-table for the ('state1', 'action1') pair\n",
    "update_q_table(0,0,5,1,1)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "      \t# Execute the action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Choose the next action randomly\n",
    "        next_action = env.action_space.sample()\n",
    "        # Update the Q-table\n",
    "        update_q_table(state, action, reward, next_state, next_action)\n",
    "        state, action = next_state, next_action   \n",
    "render_policy(get_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['action1', 'action2'] \n",
    "def update_q_table(state, action, reward, next_state):\n",
    "  \t# Get the old value of the current state-action pair\n",
    "    old_value = Q[state, action]\n",
    "    # Determine the maximum Q-value for the next state\n",
    "    next_max = max(Q[next_state])\n",
    "    # Compute the new value of the current state-action pair\n",
    "    Q[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "Q = np.array([[10, 8], [20, 15]], dtype='float32')\n",
    "# Update the Q-table\n",
    "update_q_table(0, 0, 5, 1)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10000):\n",
    "    state, info = env.reset()\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        action = env.action_space.sample()\n",
    "        # Execute the action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # Update the Q-table\n",
    "        update_q_table(state, action, reward, next_state) \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    # Append the total reward to the rewards list    \n",
    "    rewards_per_episode.append(total_reward)\n",
    "print(\"Average reward per random episode: \", np.mean(rewards_per_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(10000):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    episode_reward = 0\n",
    "    while not terminated:\n",
    "        # Select the best action based on learned Q-table\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "    reward_per_learned_episode.append(episode_reward)\n",
    "# Compute and print the average reward per learned episode\n",
    "avg_reward_per_learned_episode = np.mean(reward_per_learned_episode)\n",
    "print(\"Average reward per learned episode: \", avg_reward_per_learned_episode)\n",
    "print(\"Average reward per random episode: \", avg_reward_per_random_episode)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
