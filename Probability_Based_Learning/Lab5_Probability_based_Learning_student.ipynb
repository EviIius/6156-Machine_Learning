{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TnCTgGc8nLwC"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets"]},{"cell_type":"code","source":["#This is an interactive 3D scatter plot to visualize the Iris dataset\n","import seaborn as sns\n","\"\"\"FILL_IN\"\"\"\n","iris = sns.load_dataset(\"iris\")\n","# Plotting in 3D by plotly.express that would show the plot with capability of zooming,\n","# changing the orientation, and rotating\n","scatter_3d(iris, x='sepal_length', y='sepal_width', z='petal_length', size=\"petal_width\",\n","                   color=\"species\", color_discrete_map={\"Joly\": \"blue\", \"Bergeron\": \"violet\", \"Coderre\": \"pink\"}).show()"],"metadata":{"id":"yOoWhYBoxPCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import iris dataset\n","iris = datasets.load_iris()\n","X = iris.data[:, :2] # we only take the first two features.\n","y = iris.target"],"metadata":{"id":"J5bEFmLy9Ynl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we create an instance of SVM and fit out data. We do not scale our\n","# data since we want to plot the support vectors\n","C = 1.0 # SVM regularization parameter\n","svc = \"\"\"FILL IN\"\"\".fit(X, y)"],"metadata":{"id":"BnAXjmcDnZp1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a mesh to plot\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","h = (x_max / x_min)/100 #Step size\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))"],"metadata":{"id":"9HnSzGFPnZkH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.subplot(1, 1, 1)\n","Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) ##flatten the mesh grid arrays xx and yy into 1D arrays and concatenate them. Makes predictions based on the inputs.\n","Z = Z.reshape(xx.shape)\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) #draws filled contour plots of the decision boundaries based on the predicted labels (Z)\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","plt.xlim(xx.min(), xx.max())\n","plt.title('SVC with linear kernel')\n","plt.show()"],"metadata":{"id":"JnvOChNCnez6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These lines together plot the decision boundary and the data points on the same plot, allows you to visually assess how well the SVM classifier separates different classes."],"metadata":{"id":"PHlZddv2DLjx"}},{"cell_type":"markdown","source":["**Change Kernel fuction to see the impact.**"],"metadata":{"id":"YG9kS9Xln8GU"}},{"cell_type":"code","source":["#Radial basis function (RBF) kernel is used\n","svc = svm.SVC(\"\"\"FILL IN\"\"\").fit(X, y) #gamma and C are a hyperparameter"],"metadata":{"id":"CD4MJ5BWnmEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a mesh to plot in\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","h = (x_max / x_min)/100\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n"," np.arange(y_min, y_max, h))\n","\n","#Plot the results\n","plt.subplot(1, 1, 1)\n","Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) #flatten the mesh grid arrays xx and yy into 1D arrays\n","Z = Z.reshape(xx.shape)\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","plt.xlim(xx.min(), xx.max())\n","plt.title('gamma=0.5'+' SVC with rbf kernel')\n","plt.show()"],"metadata":{"id":"D7DwCNKroKhY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In Support Vector Machine (SVM) classifiers with a radial basis function (RBF) kernel, the decision boundary tends to be nonlinear and can take on various shapes depending on the complexity of the dataset and the parameters of the model."],"metadata":{"id":"efPPWNiAD7kP"}},{"cell_type":"markdown","source":["Plots for different values of gamma to see the effect."],"metadata":{"id":"L_5-sP7xrnCB"}},{"cell_type":"code","source":["g_list=\"\"\"FILL IN\"\"\"\n","for g in g_list:\n","  svc = svm.SVC(kernel='rbf', C=1,gamma=\"\"\"FILL IN\"\"\").fit(X, y)\n","  # create a mesh to plot in\n","  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","  h = (x_max / x_min)/100\n","  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","  np.arange(y_min, y_max, h))\n","  plt.subplot(1, 1, 1)\n","  Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n","  Z = Z.reshape(xx.shape)\n","  plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n","  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","  plt.xlabel('Sepal length')\n","  plt.ylabel('Sepal width')\n","  plt.xlim(xx.min(), xx.max())\n","  plt.title('gamma= '+str(g)+' SVC with rbf kernel')\n","  plt.show()\n"],"metadata":{"id":"_65xegWeqEf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A higher value of gamma leads to a more complex decision boundary, as it increases the influence of individual data points, resulting in a tighter fit to the training data. This can lead to overfitting if gamma is too high.\n","Conversely, a lower value of gamma results in a smoother decision boundary, which may generalize better to unseen data but may also lead to underfitting if gamma is too low."],"metadata":{"id":"Wab6lnVcEM7M"}},{"cell_type":"markdown","source":["**Effect of penalty parameter C.**"],"metadata":{"id":"TIU2YLq-r1Ac"}},{"cell_type":"code","source":["C_list=\"\"\"FILL IN\"\"\"\n","for c in C_list:\n","  svc = svm.SVC(kernel='rbf', C=\"\"\"FILL_IN\"\"\",gamma=0.5).fit(X, y)\n","  # create a mesh to plot in\n","  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","  h = (x_max / x_min)/100\n","  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","  np.arange(y_min, y_max, h))\n","  plt.subplot(1, 1, 1)\n","  Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n","  Z = Z.reshape(xx.shape)\n","  plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n","  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","  plt.xlabel('Sepal length')\n","  plt.ylabel('Sepal width')\n","  plt.xlim(xx.min(), xx.max())\n","  plt.title('C= '+str(c)+' SVC with rbf kernel')\n","  plt.show()\n"],"metadata":{"id":"NFZERNdsr4Ha"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The regularization parameter C controls the trade-off between maximizing the margin and minimizing the classification error.\n","A smaller C allows for a softer margin, which may result in a smoother decision boundary that is less influenced by individual data points.\n","A larger C imposes a harder margin, potentially leading to a more complex decision boundary that closely follows the training data."],"metadata":{"id":"ettf1MibEXar"}},{"cell_type":"markdown","source":["#Naive Bayes"],"metadata":{"id":"mV5KBwsd1NNX"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data[:, :2], iris.target\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n"],"metadata":{"id":"NqSuhks8zGdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a Gaussian Naive Bayes classifier\n","gnb = \"\"\"FILL IN\"\"\" #'var_smoothing' is a smoothing parameter that adds a fraction of the largest variance of all features to the variances for calculation stability.\n","\n","# Train the classifier on the training data\n","gnb.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = gnb.predict(X_test)\n","\n"],"metadata":{"id":"1tqmWDFQ1G74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the accuracy of the classifier\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","\n"],"metadata":{"id":"bAAWa03p1JXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the decision boundaries (2D visualization)\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n","Z = gnb.predict(np.c_[xx.ravel(), yy.ravel()])\n","Z = Z.reshape(xx.shape)\n","\n"],"metadata":{"id":"8wNllKE-1Kt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.contourf(xx, yy, Z, alpha=0.8)\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","plt.xlabel(\"Sepal Length\")\n","plt.ylabel(\"Sepal Width\")\n","plt.title(\"Gaussian Naive Bayes Decision Boundaries (2D)\")\n","plt.show()"],"metadata":{"id":"oWUvkv6q1L6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Grid Search"],"metadata":{"id":"KGxD3H9GFykL"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","nb_classifier = GaussianNB()\n","\n","params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n","gs_NB = GridSearchCV(estimator=\"\"\"FILL IN\"\"\",\n","                 param_grid=params_NB,\n","                 cv=3,   # use any cross validation technique\n","                 verbose=1,\n","                 scoring='accuracy')\n","gs_NB.fit(X_train, y_train)\n","\n","gs_NB.\"\"\"FILL IN\"\"\"\n","gs_NB.best_score_"],"metadata":{"id":"5LeaO62h0ZzX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets see how decision boundary changes with var_smoothing"],"metadata":{"id":"IeCAhmSYFtvk"}},{"cell_type":"code","source":["for i in [1,10,100]:\n"," # Create a Gaussian Naive Bayes classifier\n","        gnb = GaussianNB(var_smoothing=i)\n","\n","        # Train the classifier on the training data\n","        gnb.fit(X_train, y_train)\n","\n","        # Make predictions on the test data\n","        y_pred = gnb.predict(X_test)\n","        # Calculate the accuracy of the classifier\n","        accuracy = accuracy_score(y_test, y_pred)\n","        print(f\"Accuracy: {accuracy:.2f}\")\n","        # Visualize the decision boundaries (2D visualization)\n","        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","        xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n","        Z = gnb.predict(np.c_[xx.ravel(), yy.ravel()])\n","        Z = Z.reshape(xx.shape)\n","        plt.contourf(xx, yy, Z, alpha=0.8)\n","        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","        plt.xlabel(\"Sepal Length\")\n","        plt.ylabel(\"Sepal Width\")\n","        plt.title('var_smoothing='+str(i)+\" Gaussian Naive Bayes Decision Boundaries (2D)\")\n","        plt.show()"],"metadata":{"id":"aqb8iv9vFCRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By adding a small amount to the variances, var_smoothing can help control overfitting, especially in cases where the training data is limited or noisy.\n","Higher values of var_smoothing tend to result in a smoother decision boundary, which may improve generalization performance on unseen data but may also lead to a loss of precision and can lead to underfitting"],"metadata":{"id":"76sCtM4yGTR7"}}]}